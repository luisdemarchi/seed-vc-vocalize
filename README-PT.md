# Seed-VC
[![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Demo-blue)](https://huggingface.co/spaces/Plachta/Seed-VC)  [![arXiv](https://img.shields.io/badge/arXiv-2411.09943-%3CCOLOR%3E.svg)](https://arxiv.org/abs/2411.09943)

*[Portugu√™s (Brasil)] | [English](README.md) | [ÁÆÄ‰Ωì‰∏≠Êñá](README-ZH.md) | [Êó•Êú¨Ë™û](README-JA.md)*

[real-time-demo.webm](https://github.com/user-attachments/assets/86325c5e-f7f6-4a04-8695-97275a5d046c)

O modelo atualmente lan√ßado suporta *convers√£o de voz zero-shot* üîä, *convers√£o de voz em tempo real zero-shot* üó£Ô∏è e *convers√£o de voz de canto zero-shot* üé∂. Sem qualquer treinamento, ele √© capaz de clonar uma voz a partir de uma amostra de √°udio de refer√™ncia de 1 a 30 segundos.

Oferecemos suporte para fine-tuning (ajuste fino) com dados personalizados para melhorar o desempenho em um ou mais locutores espec√≠ficos, com requisitos de dados extremamente baixos **(m√≠nimo de 1 amostra por locutor)** e velocidade de treinamento extremamente r√°pida **(m√≠nimo de 100 passos, 2 min em uma GPU T4)**!

A **convers√£o de voz em tempo real** √© suportada, com um atraso de algoritmo de ~300ms e um atraso do lado do dispositivo de ~100ms, adequado para reuni√µes online, jogos e transmiss√µes ao vivo.

Para encontrar uma lista de demonstra√ß√µes e compara√ß√µes com modelos de convers√£o de voz anteriores, visite nossa [p√°gina de demonstra√ß√£o](https://plachtaa.github.io/seed-vc/)üåê e nossa [p√°gina de Avalia√ß√£o](EVAL.md)üìä.

Estamos continuamente melhorando a qualidade do modelo e adicionando mais recursos.

## Avalia√ß√£oüìä
Consulte [EVAL.md](EVAL.md) para resultados de avalia√ß√£o objetiva e compara√ß√µes com outras baselines.

## Instala√ß√£oüì•
Suporte m√≠nimo: Python 3.12. Foco em GPU (CUDA).

- Se quiser rodar em ambiente local, use:
  ```bash
  pip install -r requirements.txt
  ```
  Observa√ß√£o: o `requirements.txt` N√ÉO instala `torch/torchvision/torchaudio`. Para GPU, recomendamos usar Docker abaixo (instala as wheels CUDA corretas). Para ambiente local fora de Docker, instale PyTorch CUDA manualmente conforme sua GPU/driver.

- Requisitos de sistema para I/O de √°udio:
  - `ffmpeg` para reamostragem/convers√£o (obrigat√≥rio se voc√™ usar as flags de pr√©-processamento abaixo).
  - `libsndfile` (instalado automaticamente com `soundfile`).

Requisitos de sistema para I/O de √°udio:
- `ffmpeg` para reamostragem/convers√£o (obrigat√≥rio se voc√™ usar as flags de pr√©-processamento abaixo).
- `libsndfile` (instalado automaticamente com `soundfile`).

Para usu√°rios de Windows, voc√™ pode considerar instalar `triton-windows` para habilitar o uso de `--compile`, o que acelera os modelos V2:
```bash
pip install triton-windows==3.2.0.post13
```

## Usoüõ†Ô∏è
Lan√ßamos 4 modelos para diferentes finalidades:

| Vers√£o | Nome                                                                                                                                                                                                                       | Finalidade                     | Taxa de Amostragem | Codificador de Conte√∫do                                                | Vocoder | Dim Oculta | N Camadas | Par√¢metros         | Observa√ß√µes                                            |
|--------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------|--------------------|------------------------------------------------------------------------|---------|------------|-----------|--------------------|--------------------------------------------------------|
| v1.0   | seed-uvit-tat-xlsr-tiny ([ü§ó](https://huggingface.co/Plachta/Seed-VC/blob/main/DiT_uvit_tat_xlsr_ema.pth)[üìÑ](configs/presets/config_dit_mel_seed_uvit_xlsr_tiny.yml))                                                     | Convers√£o de Voz (VC)          | 22050              | XLSR-large                                                             | HIFT    | 384        | 9         | 25M                | adequado para convers√£o de voz em tempo real           |
| v1.0   | seed-uvit-whisper-small-wavenet ([ü§ó](https://huggingface.co/Plachta/Seed-VC/blob/main/DiT_seed_v2_uvit_whisper_small_wavenet_bigvgan_pruned.pth)[üìÑ](configs/presets/config_dit_mel_seed_uvit_whisper_small_wavenet.yml)) | Convers√£o de Voz (VC)          | 22050              | Whisper-small                                                          | BigVGAN | 512        | 13        | 98M                | adequado para convers√£o de voz offline                 |
| v1.0   | seed-uvit-whisper-base ([ü§ó](https://huggingface.co/Plachta/Seed-VC/blob/main/DiT_seed_v2_uvit_whisper_base_f0_44k_bigvgan_pruned_ft_ema.pth)[üìÑ](configs/presets/config_dit_mel_seed_uvit_whisper_base_f0_44k.yml))       | Convers√£o de Voz de Canto (SVC) | 44100              | Whisper-small                                                          | BigVGAN | 768        | 17        | 200M               | forte desempenho zero-shot, convers√£o de voz de canto  |
| v2.0   | hubert-bsqvae-small ([ü§ó](https://huggingface.co/Plachta/Seed-VC/blob/main/v2)[üìÑ](configs/v2/vc_wrapper.yaml))                                                                                                            | Convers√£o de Voz e Sotaque (VC) | 22050              | [ASTRAL-Quantization](https://github.com/Plachtaa/ASTRAL-quantization) | BigVGAN | 512        | 13        | 67M(CFM) + 90M(AR) | Melhor em suprimir caracter√≠sticas do locutor original |

Os checkpoints do lan√ßamento mais recente do modelo ser√£o baixados automaticamente na primeira execu√ß√£o da infer√™ncia.
Se voc√™ n√£o conseguir acessar o Hugging Face por motivos de rede, tente usar um espelho adicionando `HF_ENDPOINT=https://hf-mirror.com` antes de cada comando.

Infer√™ncia por linha de comando:
```bash
python inference.py --source <audio-de-origem> \
--target <audio-de-referencia> \
--output <diretorio-de-saida> \
--diffusion-steps 25 # recomendado 30~50 para convers√£o de voz de canto
--length-adjust 1.0 \
--inference-cfg-rate 0.7 \
--f0-condition False # defina como True para convers√£o de voz de canto
--auto-f0-adjust False # defina como True para ajustar automaticamente o tom da origem para o n√≠vel do tom do alvo, normalmente n√£o usado em convers√£o de voz de canto
--semi-tone-shift 0 # deslocamento de tom em semitons para convers√£o de voz de canto
--checkpoint <caminho-para-checkpoint> \
--config <caminho-para-config> \
 --fp16 True
```
onde:
- `source`: caminho para o arquivo de √°udio a ser convertido para a voz de refer√™ncia
- `target`: caminho para o arquivo de √°udio como refer√™ncia de voz
- `output`: caminho para o diret√≥rio de sa√≠da
- `diffusion-steps`: n√∫mero de passos de difus√£o a serem usados, padr√£o √© 25, use 30-50 para melhor qualidade, use 4-10 para infer√™ncia mais r√°pida
- `length-adjust`: fator de ajuste de dura√ß√£o, padr√£o √© 1.0, defina <1.0 para acelerar a fala, >1.0 para desacelerar
- `inference-cfg-rate`: tem uma diferen√ßa sutil na sa√≠da, padr√£o √© 0.7
- `f0-condition`: flag para condicionar o tom da sa√≠da ao tom do √°udio de origem, padr√£o √© False, defina como True para convers√£o de voz de canto
- `auto-f0-adjust`: flag para ajustar automaticamente o tom da origem para o n√≠vel do tom do alvo, padr√£o √© False, normalmente n√£o usado em convers√£o de voz de canto
- `semi-tone-shift`: deslocamento de tom em semitons para convers√£o de voz de canto, padr√£o √© 0
- `checkpoint`: caminho para o checkpoint do modelo se voc√™ treinou ou ajustou seu pr√≥prio modelo, deixe em branco para baixar automaticamente o modelo padr√£o do Hugging Face (`seed-uvit-whisper-small-wavenet` se `f0-condition` for `False`, sen√£o `seed-uvit-whisper-base`)
- `config`: caminho para a configura√ß√£o do modelo se voc√™ treinou ou ajustou seu pr√≥prio modelo, deixe em branco para baixar automaticamente a configura√ß√£o padr√£o do Hugging Face
- `fp16`: flag para usar infer√™ncia em float16, padr√£o √© True
- `preprocess-source-ffmpeg`: se True, reamostra o source para 22.05 kHz mono via ffmpeg antes da infer√™ncia (padr√£o False)
- `preprocess-target-ffmpeg`: se True, reamostra o target para 22.05 kHz mono via ffmpeg antes da infer√™ncia (padr√£o False)

Nota:
- A V1 depende do Descript Audio Codec (DAC). J√° inclu√≠mos `descript-audio-codec==1.0.0` em `requirements-py313.txt`.
- Se habilitar as flags de pr√©-processamento, garanta `ffmpeg` instalado no sistema ou use a imagem Docker fornecida (j√° inclui ffmpeg).

## Docker (GPU, Python 3.12)
Recomendado para produ√ß√£o/servidor com GPU. A imagem `Dockerfile.gpu` instala Python 3.12, PyTorch CUDA 12.1 (torch/vision/audio) e as depend√™ncias de projeto do `requirements.txt`.

### Construir a imagem (GPU)
```bash
docker build -f Dockerfile.gpu -t seed-vc:gpu .
```

### Rodar infer√™ncia V1 (exemplo)
```bash
docker run --rm --gpus all -v "$PWD:/app" seed-vc:gpu \
  python inference.py \
    --source examples/source/source_s1.wav \
    --target examples/reference/s1p1.wav \
    --output output \
    --diffusion-steps 30 \
    --inference-cfg-rate 0.7 \
    --length-adjust 1.0 \
    --f0-condition False \
    --auto-f0-adjust False
```

### docker compose (GPU)
```bash
docker compose build
docker compose run --rm seed-vc \
  python inference.py \
    --source examples/source/source_s1.wav \
    --target examples/reference/s1p1.wav \
    --output output \
    --diffusion-steps 30 \
    --inference-cfg-rate 0.7 \
    --length-adjust 1.0 \
    --f0-condition False \
    --auto-f0-adjust False
```

Notas:
- O projeto usa um √öNICO `requirements.txt` compartilhado. As bibliotecas `torch/torchvision/torchaudio` s√£o instaladas no `Dockerfile.gpu` com CUDA 12.1, e por isso n√£o aparecem no `requirements.txt`.
- Entrada/sa√≠da de √°udio via `soundfile`/`libsndfile`; `ffmpeg` j√° vem instalado na imagem para reamostragem/convers√£o.
- Ao montar o reposit√≥rio com `-v "$PWD:/app"`, os arquivos gerados aparecem no seu diret√≥rio local `output/`.

Da mesma forma, para usar o modelo V2, voc√™ pode executar:
```bash
python inference_v2.py --source <audio-de-origem> \
--target <audio-de-referencia> \
--output <diretorio-de-saida> \
--diffusion-steps 25 # recomendado 30~50 para convers√£o de voz de canto
--length-adjust 1.0 # igual ao V1
--intelligibility-cfg-rate 0.7 # controla qu√£o claro √© o conte√∫do lingu√≠stico da sa√≠da, recomendado 0.0~1.0
--similarity-cfg-rate 0.7 # controla qu√£o semelhante a voz de sa√≠da √© √† voz de refer√™ncia, recomendado 0.0~1.0
--convert-style true # se deve usar o modelo AR para convers√£o de sotaque e emo√ß√£o, definir como false far√° apenas a convers√£o de timbre, semelhante ao V1
--anonymization-only false # definir como true ignorar√° o √°udio de refer√™ncia, mas apenas anonimizar√° a fala de origem para uma "voz m√©dia"
--top-p 0.9 # controla a diversidade da sa√≠da do modelo AR, recomendado 0.5~1.0
--temperature 1.0 # controla a aleatoriedade da sa√≠da do modelo AR, recomendado 0.7~1.2
--repetition-penalty 1.0 # penaliza a repeti√ß√£o da sa√≠da do modelo AR, recomendado 1.0~1.5
--cfm-checkpoint-path <caminho-para-checkpoint-cfm> # caminho para o checkpoint do modelo CFM, deixe em branco para baixar automaticamente o modelo padr√£o do Hugging Face
--ar-checkpoint-path <caminho-para-checkpoint-ar> # caminho para o checkpoint do modelo AR, deixe em branco para baixar automaticamente o modelo padr√£o do Hugging Face
```

## Interface Web de Convers√£o de Voz (V1)
```bash
python app_vc.py --checkpoint <caminho-para-checkpoint> --config <caminho-para-config> --fp16 True
```
- `checkpoint`: caminho para o checkpoint do modelo se voc√™ treinou ou ajustou seu pr√≥prio modelo, deixe em branco para baixar automaticamente o modelo padr√£o do Hugging Face (`seed-uvit-whisper-small-wavenet`)
- `config`: caminho para a configura√ß√£o do modelo se voc√™ treinou ou ajustou seu pr√≥prio modelo, deixe em branco para baixar automaticamente a configura√ß√£o padr√£o do Hugging Face

Em seguida, abra o navegador e acesse `http://localhost:7860/` para usar a interface web.

## Interface Web de Convers√£o de Voz de Canto (V1‚Äëf0)
```bash
python app_svc.py --checkpoint <caminho-para-checkpoint> --config <caminho-para-config> --fp16 True
```
- `checkpoint`: caminho para o checkpoint do modelo se voc√™ treinou ou ajustou seu pr√≥prio modelo, deixe em branco para baixar automaticamente o modelo padr√£o do Hugging Face (`seed-uvit-whisper-base`)
- `config`: caminho para a configura√ß√£o do modelo se voc√™ treinou ou ajustou seu pr√≥prio modelo, deixe em branco para baixar automaticamente a configura√ß√£o padr√£o do Hugging Face

## Interface Web do modelo V2
```bash
python app_vc_v2.py --cfm-checkpoint-path <caminho-para-checkpoint-cfm> --ar-checkpoint-path <caminho-para-checkpoint-ar>
```
- `cfm-checkpoint-path`: caminho para o checkpoint do modelo CFM, deixe em branco para baixar automaticamente o modelo padr√£o do Hugging Face
- `ar-checkpoint-path`: caminho para o checkpoint do modelo AR, deixe em branco para baixar automaticamente o modelo padr√£o do Hugging Face
- voc√™ pode considerar adicionar `--compile` para obter uma acelera√ß√£o de ~6x na infer√™ncia do modelo AR

## Interface Web Integrada
```bash
python app.py --enable-v1 --enable-v2
```
Isso carregar√° apenas modelos pr√©-treinados para infer√™ncia zero-shot. Para usar checkpoints personalizados, execute `app_vc.py` ou `app_svc.py` como acima.
Se voc√™ tiver mem√≥ria limitada, remova `--enable-v2` ou `--enable-v1` para carregar apenas um dos conjuntos de modelos.

GUI de convers√£o de voz em tempo real:
```bash
python real-time-gui.py --checkpoint-path <caminho-para-checkpoint> --config-path <caminho-para-config>
```
- `checkpoint`: caminho para o checkpoint do modelo se voc√™ treinou ou ajustou seu pr√≥prio modelo, deixe em branco para baixar automaticamente o modelo padr√£o do Hugging Face (`seed-uvit-tat-xlsr-tiny`)
- `config`: caminho para a configura√ß√£o do modelo se voc√™ treinou ou ajustou seu pr√≥prio modelo, deixe em branco para baixar automaticamente a configura√ß√£o padr√£o do Hugging Face

> [!IMPORTANT]
> √â altamente recomend√°vel usar uma GPU para convers√£o de voz em tempo real.
> Alguns testes de desempenho foram feitos em uma GPU NVIDIA RTX 3060 Laptop, os resultados e as configura√ß√µes de par√¢metros recomendadas est√£o listados abaixo:

| Configura√ß√£o do Modelo          | Passos de Difus√£o | Taxa de CFG de Infer√™ncia | Comprimento M√°x. do Prompt | Tempo de Bloco (s) | Dura√ß√£o do Crossfade (s) | Contexto Extra (esquerda) (s) | Contexto Extra (direita) (s) | Lat√™ncia (ms) | Tempo de Infer√™ncia por Bloco (ms) |
|---------------------------------|-------------------|---------------------------|----------------------------|--------------------|--------------------------|-------------------------------|------------------------------|---------------|------------------------------------|
| seed-uvit-xlsr-tiny             | 10                | 0.7                       | 3.0                        | 0.18s              | 0.04s                    | 2.5s                          | 0.02s                        | 430ms         | 150ms                              |

Voc√™ pode ajustar os par√¢metros na GUI de acordo com o desempenho do seu pr√≥prio dispositivo, o fluxo de convers√£o de voz deve funcionar bem desde que o Tempo de Infer√™ncia seja menor que o Tempo de Bloco.
Observe que a velocidade de infer√™ncia pode diminuir se voc√™ estiver executando outras tarefas intensivas de GPU (por exemplo, jogos, assistir a v√≠deos).

Explica√ß√µes para os par√¢metros da GUI de convers√£o de voz em tempo real:
- `Diffusion Steps`: n√∫mero de passos de difus√£o a serem usados, no caso de tempo real, geralmente definido como 4~10 para a infer√™ncia mais r√°pida;
- `Inference CFG Rate`: tem uma diferen√ßa sutil na sa√≠da, o padr√£o √© 0.7, definir como 0.0 ganha cerca de 1.5x de acelera√ß√£o;
- `Max Prompt Length`: comprimento m√°ximo do √°udio de prompt, definir um valor baixo pode acelerar a infer√™ncia, mas pode reduzir a semelhan√ßa com a fala do prompt;
- `Block Time`: dura√ß√£o de cada bloco de √°udio para infer√™ncia, quanto maior o valor, maior a lat√™ncia, observe que este valor deve ser maior que o tempo de infer√™ncia por bloco, defina de acordo com a condi√ß√£o do seu hardware;
- `Crossfade Length`: dura√ß√£o do crossfade entre os blocos de √°udio, normalmente n√£o precisa ser alterado;
- `Extra context (left)`: dura√ß√£o do contexto hist√≥rico extra para infer√™ncia, quanto maior o valor, maior o tempo de infer√™ncia, mas pode aumentar a estabilidade;
- `Extra context (right)`: dura√ß√£o do contexto futuro extra para infer√™ncia, quanto maior o valor, maior o tempo de infer√™ncia e a lat√™ncia, mas pode aumentar a estabilidade;

O atraso do algoritmo √© calculado aproximadamente como `Tempo de Bloco * 2 + Contexto extra (direita)`, o atraso do lado do dispositivo √© geralmente de ~100ms. O atraso geral √© a soma dos dois.

Voc√™ pode usar o [VB-CABLE](https://vb-audio.com/Cable/) para rotear o √°udio do fluxo de sa√≠da da GUI para um microfone virtual.

*(A GUI e a l√≥gica de fragmenta√ß√£o de √°udio foram modificadas do [RVC](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI), obrigado pela implementa√ß√£o brilhante!)*

## Treinamento (resumo)
O fine-tuning com dados personalizados permite que o modelo clone a voz de algu√©m com mais precis√£o. Isso melhorar√° muito a semelhan√ßa do locutor em locutores espec√≠ficos, mas pode aumentar ligeiramente a WER (Taxa de Erro de Palavra).
Um Tutorial do Colab est√° aqui para voc√™ seguir: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1R1BJTqMsTXZzYAVx3j1BiemFXog9pbQG?usp=sharing)

1. Prepare seu pr√≥prio conjunto de dados. Ele deve satisfazer o seguinte:
    - A estrutura dos arquivos n√£o importa
    - Cada arquivo de √°udio deve ter entre 1 e 30 segundos, caso contr√°rio, ser√° ignorado
    - Todos os arquivos de √°udio devem estar em um dos seguintes formatos: `.wav` `.flac` `.mp3` `.m4a` `.opus` `.ogg`
    - A etiqueta do locutor n√£o √© necess√°ria, mas certifique-se de que cada locutor tenha pelo menos 1 amostra
    - Claro, quanto mais dados voc√™ tiver, melhor ser√° o desempenho do modelo
    - Os dados de treinamento devem ser o mais limpos poss√≠vel, m√∫sica de fundo ou ru√≠do n√£o s√£o desejados
2. Escolha um arquivo de configura√ß√£o de modelo de `configs/presets/` para fine-tuning, ou crie o seu pr√≥prio para treinar do zero.
    - Para fine-tuning, deve ser um dos seguintes:
        - `./configs/presets/config_dit_mel_seed_uvit_xlsr_tiny.yml` para convers√£o de voz em tempo real
        - `./configs/presets/config_dit_mel_seed_uvit_whisper_small_wavenet.yml` para convers√£o de voz offline
        - `./configs/presets/config_dit_mel_seed_uvit_whisper_base_f0_44k.yml` para convers√£o de voz de canto
3. Execute o seguinte comando para iniciar o treinamento:
```bash
python train.py \
--config <caminho-para-config> \
--dataset-dir <caminho-para-dados> \
--run-name <nome-da-execucao> \
--batch-size 2 \
--max-steps 1000 \
--max-epochs 1000 \
--save-every 500 \
--num-workers 0
```
onde:
- `config`: caminho para a configura√ß√£o do modelo, escolha um dos acima para fine-tuning ou crie o seu pr√≥prio para treinar do zero
- `dataset-dir`: caminho para o diret√≥rio do conjunto de dados, que deve ser uma pasta contendo todos os arquivos de √°udio
- `run-name`: nome da execu√ß√£o, que ser√° usado para salvar os checkpoints e logs do modelo
- `batch-size`: tamanho do lote para treinamento, escolha dependendo da mem√≥ria da sua GPU.
- `max-steps`: n√∫mero m√°ximo de passos para treinar, escolha dependendo do tamanho do seu conjunto de dados e do tempo de treinamento
- `max-epochs`: n√∫mero m√°ximo de √©pocas para treinar, escolha dependendo do tamanho do seu conjunto de dados e do tempo de treinamento
- `save-every`: n√∫mero de passos para salvar o checkpoint do modelo
- `num-workers`: n√∫mero de workers para carregamento de dados, defina como 0 para Windows

Da mesma forma, para treinar o modelo V2, voc√™ pode executar: (observe que o script de treinamento V2 suporta treinamento multi-GPU)
```bash
accelerate launch train_v2.py \
--dataset-dir <caminho-para-dados> \
--run-name <nome-da-execucao> \
--batch-size 2 \
--max-steps 1000 \
--max-epochs 1000 \
--save-every 500 \
--num-workers 0 \
--train-cfm
```

4. Se o treinamento parar acidentalmente, voc√™ pode retom√°-lo executando o mesmo comando novamente, o treinamento continuar√° do √∫ltimo checkpoint. (Certifique-se de que os argumentos `run-name` e `config` sejam os mesmos para que o √∫ltimo checkpoint possa ser encontrado)

5. Ap√≥s o treinamento, voc√™ pode usar o modelo treinado para infer√™ncia, especificando o caminho para o checkpoint e o arquivo de configura√ß√£o.
    - Eles devem estar em `./runs/<nome-da-execucao>/`, com o checkpoint nomeado `ft_model.pth` e o arquivo de configura√ß√£o com o mesmo nome do arquivo de configura√ß√£o de treinamento.
    - Voc√™ ainda precisa especificar um arquivo de √°udio de refer√™ncia do locutor que deseja usar durante a infer√™ncia, semelhante ao uso zero-shot.

## TODOüìù
- [x] Lan√ßar c√≥digo
- [x] Lan√ßar modelos pr√©-treinados: [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-SeedVC-blue)](https://huggingface.co/Plachta/Seed-VC)
- [x] Demo no Hugging Face Space: [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Space-blue)](https://huggingface.co/spaces/Plachta/Seed-VC)
- [x] P√°gina de demonstra√ß√£o HTML: [Demo](https://plachtaa.github.io/seed-vc/)
- [x] Infer√™ncia em streaming
- [x] Reduzir a lat√™ncia da infer√™ncia em streaming
- [x] V√≠deo de demonstra√ß√£o para convers√£o de voz em tempo real
- [x] Convers√£o de voz de canto
- [x] Resili√™ncia a ru√≠do para √°udio de origem
- [ ] Melhorias potenciais na arquitetura
    - [x] Conex√µes de salto no estilo U-ViT
    - [x] Mudan√ßa da entrada para OpenAI Whisper
    - [x] Tempo como Token
- [x] C√≥digo para treinamento com dados personalizados
- [x] Fine-tuning de locutor few-shot/one-shot
- [x] Mudan√ßa para BigVGAN da NVIDIA para decodifica√ß√£o de voz de canto
- [x] Modelo de vers√£o Whisper para convers√£o de voz de canto
- [x] Avalia√ß√£o objetiva e compara√ß√£o com RVC/SoVITS para convers√£o de voz de canto
- [x] Melhorar a qualidade do √°udio
- [ ] Vocoder NSF para melhor convers√£o de voz de canto
- [x] Corrigir artefato de convers√£o de voz em tempo real ao n√£o falar (feito adicionando um modelo VAD)
- [x] Notebook Colab para exemplo de fine-tuning
- [x] Substituir o Whisper por um extrator de conte√∫do lingu√≠stico mais avan√ßado
- [ ] Mais a ser adicionado
- [x] Adicionar suporte para Apple Silicon
- [ ] Lan√ßar artigo, avalia√ß√µes e p√°gina de demonstra√ß√£o para o modelo V2

## Problemas Conhecidos
- No Mac - executar `real-time-gui.py` pode gerar um erro `ModuleNotFoundError: No module named '_tkinter'`, neste caso, uma nova vers√£o do Python **com suporte a Tkinter** deve ser instalada. Consulte [Este Guia no Stack Overflow](https://stackoverflow.com/questions/76105218/why-does-tkinter-or-turtle-seem-to-be-missing-or-broken-shouldnt-it-be-part) para uma explica√ß√£o do problema e uma corre√ß√£o detalhada.


## CHANGELOGSüóíÔ∏è
- 2024-04-16
    - Lan√ßado modelo V2 para convers√£o de voz e sotaque, com melhor anonimiza√ß√£o do locutor de origem
- 2025-03-03:
    - Adicionado suporte para Mac S√©rie M (Apple Silicon)
- 2024-11-26:
    - Atualizado modelo pr√©-treinado da vers√£o v1.0 tiny, otimizado para convers√£o de voz em tempo real
    - Suporte para fine-tuning de um ou m√∫ltiplos locutores one-shot/few-shot
    - Suporte para uso de checkpoint personalizado para WebUI e GUI em tempo real
- 2024-11-19:
    - Artigo do arXiv lan√ßado
- 2024-10-28:
    - Atualizado modelo de convers√£o de voz de canto de 44k com fine-tuning e melhor qualidade de √°udio
- 2024-10-27:
    - Adicionada GUI de convers√£o de voz em tempo real
- 2024-10-25:
    - Adicionados resultados de avalia√ß√£o exaustivos e compara√ß√µes com RVCv2 para convers√£o de voz de canto
- 2024-10-24:
    - Atualizado modelo de convers√£o de voz de canto de 44kHz, com OpenAI Whisper como entrada de conte√∫do de fala
- 2024-10-07:
    - Atualizado modelo pr√©-treinado v0.3, alterado o codificador de conte√∫do de fala para OpenAI Whisper
    - Adicionados resultados de avalia√ß√£o objetiva para o modelo pr√©-treinado v0.3
- 2024-09-22:
    - Atualizado modelo de convers√£o de voz de canto para usar BigVGAN da NVIDIA, proporcionando grande melhoria para vozes de canto agudas
    - Suporte para fragmenta√ß√£o e sa√≠da em streaming para arquivos de √°udio longos na Web UI
- 2024-09-18:
    - Atualizado modelo condicionado a f0 para convers√£o de voz de canto
- 2024-09-14:
    - Atualizado modelo pr√©-treinado v0.2, com tamanho menor e menos passos de difus√£o para alcan√ßar a mesma qualidade, e capacidade adicional de controlar a preserva√ß√£o da pros√≥dia
    - Adicionado script de infer√™ncia por linha de comando
    - Adicionadas instru√ß√µes de instala√ß√£o e uso

## Agradecimentosüôè
- [Amphion](https://github.com/open-mmlab/Amphion) por fornecer recursos computacionais e inspira√ß√£o!
- [Vevo](https://github.com/open-mmlab/Amphion/tree/main/models/vc/vevo) pela base te√≥rica do modelo V2
- [MegaTTS3](https://github.com/bytedance/MegaTTS3) pela implementa√ß√£o da infer√™ncia CFG multi-condi√ß√£o no modelo V2
- [ASTRAL-quantiztion](https://github.com/Plachtaa/ASTRAL-quantization) pelo incr√≠vel tokenizador de fala com desentrela√ßamento de locutor usado pelo modelo V2
- [RVC](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI) pela base da convers√£o de voz em tempo real
- [SEED-TTS](https://arxiv.org/abs/2406.02430) pela ideia inicial
